---
title: "Deep Learning"
author: "Agam Safaruddin"
---
::: callout-note
Please see 'deep-learning.ipynb' in the github work folder. Tensorflow package installation in Rstudio is having problems with the currently installed tensorflow in python in my computer. So please look at the 'deep-learning.ipynb'.
:::


# Libraries
```{r}
library(tidyverse)
library(keras)
library(lime)
library(rsample)
library(recipes)
library(yardstick)
library(corrr)
library(readxl)
library(forcats)
```


# import data
```{r}
churn_data_raw <- read_csv("datas/WA_Fn-UseC_-Telco-Customer-Churn.csv")

glimpse(churn_data_raw)
```

# preprocessing
```{r}
churn_data_tbl <- churn_data_raw %>%
                  select(-customerID) %>%
                  na.omit(churn_data_raw$TotalCharges) %>%
                  select(Churn, everything())

#churn_data_tbl
```

# split 80/20
```{r}
# Split test/training sets
set.seed(100)

train_test_split <- rsample::initial_split(churn_data_tbl, prop = 0.8)
train_test_split

## <Analysis/Assess/Total>
## <5626/1406/7032>

# Retrieve train and test sets
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
```


```{r}
churn_data_tbl %>% ggplot(aes(x = tenure)) + 
  geom_histogram(bins = 6, color = "white", fill =  "#2DC6D6") +
  labs(
    title = "Tenure Counts With Six Bins",
    x     = "tenure (month)"
  )
```


```{r}
# Determine if log transformation improves correlation 
# between TotalCharges and Churn

train_tbl %>%
    select(Churn, TotalCharges) %>%
    mutate(
        Churn = Churn %>% as.factor() %>% as.numeric(),
        LogTotalCharges = log(TotalCharges)
        ) %>%
    correlate() %>%
    focus(Churn) %>%
    fashion()
```


```{r}
churn_data_tbl %>% 
        pivot_longer(cols      = c(Contract, InternetService, MultipleLines, PaymentMethod), 
                     names_to  = "feature", 
                     values_to = "category") %>% 
        ggplot(aes(category)) +
          geom_bar(fill = "#2DC6D6") +
          facet_wrap(~ feature, scales = "free") +
          labs(
            title = "Features with multiple categories: Need to be one-hot encoded"
          ) +
          theme(axis.text.x = element_text(angle = 25, 
                                           hjust = 1))
```

# Preprocessing with recipes
```{r}
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
    step_rm(Churn) %>% 
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep(data = train_tbl)
```

# Bake
```{r}
# Predictors
x_train_tbl <- bake(rec_obj , new_data = train_tbl )
x_test_tbl  <- bake(rec_obj , new_data = test_tbl )
```


```{r}
# Response variables for training and testing sets
y_train_vec <- ifelse(churn_data_tbl$Churn == "Yes", 1, 0)
y_test_vec  <- ifelse(test_tbl$Churn == "Yes", 1, 0)
```


# ANN

# Building our Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
    # First hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu", 
        input_shape        = ncol(x_train_tbl)) %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Second hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu") %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Output layer
    layer_dense(
        units              = 1, 
        kernel_initializer = "uniform", 
        activation         = "sigmoid") %>% 
    # Compile ANN
    compile(
        optimizer = 'adam',
        loss      = 'binary_crossentropy',
        metrics   = c('accuracy')
    )
model_keras


# Training

# Fit the keras model to the training data
fit_keras <- fit(
    model_keras,
    x = as.matrix(x_train_tbl),
    y = y_train_vec,
    batch_size = 50,
    epochs = 35,
    validation_split = 0.3
)




fit_keras


# Plot

# Plot the training/validation history of our Keras model
plot(fit_keras) +
#geom_smooth(formula = y ~ x, method = "auto", se = FALSE, color = "blue") +
  labs(title = "Deep Learning Training Results") +
  theme(legend.position  = "bottom", 
        strip.placement  = "inside",
        strip.background = element_rect(fill = "grey"))


# make predictions
* tensorflow version is different, therefore the code has been adjusted

# Predicted Class
#yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#    as.vector()

# Predicted Class Probability
#yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#    as.vector()

# Predicted Class
yhat_keras_class_vec <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>%
  k_argmax() %>% as.vector()

# Predicted Class Probability
yhat_keras_prob_vec <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>% as.vector()




# Inspect performance

estimates_keras_tbl <- tibble(
    truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
    estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
    class_prob = yhat_keras_prob_vec
)

estimates_keras_tbl
